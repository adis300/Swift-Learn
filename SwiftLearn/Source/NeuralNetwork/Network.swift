//
//  Network.swift
//  Swift-Learn
//
//  Created by Disi A on 11/7/16.
//  Copyright Â© 2016 Votebin. All rights reserved.
//

import Foundation

public class Network {
    
    var numLayers : Int
    var layerSizes: [Int]
    public var weights : [Matrix<Double>] = []
    public var biases: [Vector<Double>] = []
    
    public init(_ layerSizes: [Int]) {
        
        precondition(layerSizes.count > 1, "Unable to initialize neural network with no count")
        
        numLayers = layerSizes.count
        self.layerSizes = layerSizes
        
        for layerNumber in 1..<layerSizes.count {
            // biases.append(Vector<Double>(layerSizes[nodeCount])) // Zero biases for test
            biases.append(Vector<Double>(randomLength: layerSizes[layerNumber]))
            
            // weights.append(Matrix(rows: layerSizes[nodeCount],  cols:layerSizes[nodeCount - 1]))
            weights.append(Matrix(randomSize:(layerSizes[layerNumber], layerSizes[layerNumber - 1])))
        }
    }
    
    public init(jsonData: Data) {
        
        precondition(jsonData.count > 1, "Unable to initialize neural network with empty data")
        
        do {

            let decoded = try JSONSerialization.jsonObject(with: jsonData, options: []) as! [String: Any]
            self.layerSizes = decoded["layerSizes"] as! [Int]
            numLayers = self.layerSizes.count
            let biasVectors = decoded["biases"] as! [[String: [Double]]]
            biases = biasVectors.map{Vector($0["vector"]!)}
            
            let weightMatrices = decoded["weights"] as! [[String: Any]]
            weights = weightMatrices.map{x in
                let cols = x["cols"] as! Int
                let rows = x["rows"] as! Int
                let grid = x["grid"] as! [Double]
                return Matrix(rows: rows, cols: cols, values: grid)
            }

        } catch {
            print(error.localizedDescription)
            numLayers = 0
            self.layerSizes = []
        }
        
    }
    
    // Return the output of the network if x is input.

    public func feedforward(_ x: Vector<Double>) -> Vector<Double>{
        var a = x
        for layerNumber in 1..<layerSizes.count {
            a = sigmoid(weights[layerNumber - 1] * a + biases[layerNumber - 1])
        }
        // print("Test activation: \(a)")
        return a
    }
    
    
    /*
    Return the number of test inputs for which the neural
    network outputs the correct result. Note that the neural
    network's output is assumed to be the index of whichever
    neuron in the final layer has the highest activation.
     */
    func evaluate(_ testSet: [LabeledData]) -> Int{
        var correctCount: Int = 0
        for testData in testSet{
            let (_, maxIndTest) = max(feedforward(testData.input))
            let (_, maxIndTestLabel) = max(testData.label)
            if maxIndTest == maxIndTestLabel{
                correctCount += 1
            }
        }
        return correctCount
    }
    /*
    Update the network's weights and biases by applying
    gradient descent using backpropagation to a single mini batch.
    The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
    is the learning rate. 
     */
    func updateMiniBatch(miniBatch: [LabeledData], eta: Double){
        
        if miniBatch.count == 0{
            print("Empty mini batch, no update was computed")
            return
        }
        
        var nablaB = biases.map{Vector($0.length)} // Gradient of biases
        // var nablaW = weights.map{Matrix(size: ($0.rows, $0.cols))}
        var nablaW = weights.map{Matrix(rows: $0.rows, cols:$0.cols)}
        
        for trainingData in miniBatch {
            let (deltaNablaW, deltaNablaB) = backProp(trainingData)
            for layerNumber in 1..<layerSizes.count {
                nablaB[layerNumber - 1] += deltaNablaB[layerNumber - 1]
                nablaW[layerNumber - 1] += deltaNablaW[layerNumber - 1]
            }
        }
        
        // Finally reverse update the network weights and biases by scaling the gradient with learning rate and batch size.
        for layerNumber in 1..<layerSizes.count {
            addTo(&biases[layerNumber - 1], values: nablaB[layerNumber - 1], scale: -eta/miniBatch.count)
            addTo(&weights[layerNumber - 1], values: nablaW[layerNumber - 1], scale: -eta/miniBatch.count)
        }
        // print("New weights:")
        // print(weights)
    }
    /*
     Return a tuple ``(nabla_w,nabla_b)`` representing the
     gradient for the cost function C_x.  ``nabla_b`` and
     ``nabla_w`` are layer-by-layer lists of numpy arrays, similar
     to `self.weights` and `self.biases`.
     */
    func backProp(_ dataSet:LabeledData) -> ([Matrix<Double>], [Vector<Double>]){
        
        // var nablaB = biases.map{Vector($0.length, repeatedValue: 0.1)} // Gradient of biases
        // var nablaB = biases.map{Vector($0.length)}
        // var nablaW = weights.map{Matrix(rows: $0.rows, cols: $0.cols)} // Gradient of weights
        
        // Feedforward
        var activations:[Vector<Double>] = [dataSet.input]
        var zs: [Vector<Double>] = []
        
        for layerNumber in 1..<layerSizes.count {
            let z = weights[layerNumber - 1] * (activations[layerNumber - 1]) + biases[layerNumber - 1]
            zs.append(z)
            activations.append(sigmoid(z))
        }
        
        // Back propagation passes
        let costDerivative = activations[activations.endIndex - 1] - dataSet.label
        var delta = costDerivative .* sigmoidPrime(zs[zs.endIndex - 1])
        
        var nablaBReversed = [delta]
        var nablaWReversed = [delta *^ activations[activations.endIndex - 2]]
        /*
         l = 1 means the last layer of neurons, l = 2 is the
         second-last layer, and so on.  It's a renumbering of the
        */
        for lback in 2..<layerSizes.count{
            let sp = sigmoidPrime(zs[zs.endIndex - lback])
            delta = (transpose(weights[weights.endIndex - lback + 1]) * delta) .* sp
            nablaBReversed.append(delta)
            nablaWReversed.append(delta *^ activations[activations.endIndex - lback - 1])
        }
        
        return (nablaWReversed.reversed(), nablaBReversed.reversed())
    }
    
    /* 
    Train the neural network using mini-batch stochastic
    gradient descent.  The ``training_data`` is a list of tuples
    ``(x, y)`` representing the training inputs and the desired
    outputs.  The other non-optional parameters are
    self-explanatory.  If ``test_data`` is provided then the
    network will be evaluated against the test data after each
    epoch, and partial progress printed out.  This is useful for
    tracking progress, but slows things down substantially.
    */
    
    public func SGD(trainingSet: [LabeledData], epochs: Int, miniBatchSize: Int, eta: Double, testSet: [LabeledData]?){
        for epoch in 1...epochs{ // Defines train epoch many times
            
            var trainingSet = trainingSet.shuffled()
            
            for i in stride(from: 0, to: trainingSet.count, by: miniBatchSize){
                if i + miniBatchSize > trainingSet.count{
                    let miniBatch = Array(trainingSet[i ..< trainingSet.count])
                    updateMiniBatch(miniBatch: miniBatch, eta: eta)
                }else{
                    let miniBatch = Array(trainingSet[i ..< i+miniBatchSize])
                    updateMiniBatch(miniBatch: miniBatch, eta: eta)
                }
            }
            if let testDataSet = testSet{
                print("Epoch \(epoch): \(evaluate(testDataSet)) / \(testDataSet.count)")
            }else{
                print("Epoch \(epoch) complete")
            }
        }
    }
    
    public func toJson() -> Data{
        var dict: [String: Any] = ["layerSizes": layerSizes]
        dict["weights"] = weights.map{$0.dict}
        dict["biases"] = biases.map{$0.dict}
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: dict, options: .prettyPrinted)
            return jsonData
        }catch{
            print(error.localizedDescription)
            return Data()
        }
    }
}
